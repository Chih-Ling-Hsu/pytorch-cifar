{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-train on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CIFAR10 with PyTorch.'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from models import *\n",
    "from utils import progress_bar\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "# net = VGG('VGG19')\n",
    "# net = ResNet18()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "net = MobileNetV2()\n",
    "# net = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('/tmp/work/checkpoint'):\n",
    "            os.mkdir('/tmp/work/checkpoint')\n",
    "        torch.save(state, '/tmp/work/checkpoint/ckpt.cifar10.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, 350):\n",
    "    scheduler.step()\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing accuracy: >90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on CINIC-10\n",
    "\n",
    "- train: train set\n",
    "- valid: valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CINIC10 with PyTorch.'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from models import *\n",
    "from utils import progress_bar\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.47889522, 0.47227842, 0.43047404],  std=[0.24205776, 0.23828046, 0.25874835]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.47889522, 0.47227842, 0.43047404],  std=[0.24205776, 0.23828046, 0.25874835]),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root='/tmp/work/data/CINIC-10/train/', transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root='/tmp/work/data/CINIC-10/valid/', transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "# net = VGG('VGG19')\n",
    "# net = ResNet18()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "net = MobileNetV2()\n",
    "# net = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Load checkpoint.\n",
    "print('==> Resuming from checkpoint..')\n",
    "checkpoint = torch.load('/tmp/work/checkpoint/ckpt.cifar10.t7')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "#start_epoch = checkpoint['epoch']\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Acc: %.3f%%'\n",
    "            % (100.*correct/total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Acc: %.3f%%'\n",
    "                % (100.*correct/total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('/tmp/work/checkpoint'):\n",
    "            os.mkdir('/tmp/work/checkpoint')\n",
    "        torch.save(state, '/tmp/work/checkpoint/ckpt.cinic10.0.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, 350):\n",
    "    scheduler.step()\n",
    "    train(epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy 77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on full CINIC-10\n",
    "\n",
    "- train: train set + valid set\n",
    "- valid: test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CINIC10 with PyTorch.'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from models import *\n",
    "from utils import progress_bar\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.47889522, 0.47227842, 0.43047404],  std=[0.24205776, 0.23828046, 0.25874835]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.47889522, 0.47227842, 0.43047404],  std=[0.24205776, 0.23828046, 0.25874835]),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root='/tmp/work/data/CINIC-10/train/', transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "validset = torchvision.datasets.ImageFolder(root='/tmp/work/data/CINIC-10/valid/', transform=transform_train)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root='/tmp/work/data/CINIC-10/valid/', transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "# net = VGG('VGG19')\n",
    "# net = ResNet18()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "net = MobileNetV2()\n",
    "# net = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Load checkpoint.\n",
    "print('==> Resuming from checkpoint..')\n",
    "checkpoint = torch.load('/tmp/work/checkpoint/ckpt.cinic10.0.t7')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Acc: %.3f%%'\n",
    "            % (100.*correct/total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Acc: %.3f%%'\n",
    "                % (100.*correct/total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('/tmp/work/checkpoint'):\n",
    "            os.mkdir('/tmp/work/checkpoint')\n",
    "        torch.save(state, '/tmp/work/checkpoint/ckpt.cinic10.1.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "for epoch in range(start_epoch, 350):\n",
    "    scheduler.step()\n",
    "    train(epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on full CINIC-10 with data augmentation\n",
    "\n",
    "- train: train set + valid set\n",
    "- valid: test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CINIC10 with PyTorch.'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from models import *\n",
    "from utils import progress_bar\n",
    "\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug import parameters as iap\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "class ImgAugTransform:\n",
    "    def __init__(self):\n",
    "        self.aug = iaa.Sequential([\n",
    "            iaa.SomeOf((0, 2), [\n",
    "                iaa.Affine(rotate=iap.Uniform(0.0, 360.0)),\n",
    "                iaa.CropAndPad(percent=(-0.2, 0)),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "                iaa.AdditiveGaussianNoise(scale=0.1*255),\n",
    "                iaa.Sharpen(alpha=0.5),\n",
    "                iaa.PiecewiseAffine(scale=iap.Uniform(0.02, 0.08), \n",
    "                                    nb_rows=iap.Uniform(4,8),\n",
    "                                    nb_cols=iap.Uniform(4,8))\n",
    "            ]),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        return self.aug.augment_image(img)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.47889522, 0.47227842, 0.43047404],  std=[0.24205776, 0.23828046, 0.25874835]),\n",
    "    ImgAugTransform(),\n",
    "    lambda x: PIL.Image.fromarray(x),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.47889522, 0.47227842, 0.43047404],  std=[0.24205776, 0.23828046, 0.25874835]),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root='/tmp/work/data/CINIC-10/train/', transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "validset = torchvision.datasets.ImageFolder(root='/tmp/work/data/CINIC-10/valid/', transform=transform_train)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root='/tmp/work/data/CINIC-10/valid/', transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "# net = VGG('VGG19')\n",
    "# net = ResNet18()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "net = MobileNetV2()\n",
    "# net = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Load checkpoint.\n",
    "print('==> Resuming from checkpoint..')\n",
    "checkpoint = torch.load('/tmp/work/checkpoint/ckpt.cinic10.1.t7')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Acc: %.3f%%'\n",
    "            % (100.*correct/total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Acc: %.3f%%'\n",
    "                % (100.*correct/total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('/tmp/work/checkpoint'):\n",
    "            os.mkdir('/tmp/work/checkpoint')\n",
    "        torch.save(state, '/tmp/work/checkpoint/ckpt.cinic10.2.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, 350):\n",
    "    scheduler.step()\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n",
      "==> Resuming from checkpoint..\n",
      "==> Exporting model..\n",
      "graph(%0 : Float(1, 3, 32, 32)\n",
      "      %1 : Float(32, 3, 3, 3)\n",
      "      %2 : Float(32)\n",
      "      %3 : Float(32)\n",
      "      %4 : Float(32)\n",
      "      %5 : Float(32)\n",
      "      %6 : Long()\n",
      "      %7 : Float(32, 32, 1, 1)\n",
      "      %8 : Float(32)\n",
      "      %9 : Float(32)\n",
      "      %10 : Float(32)\n",
      "      %11 : Float(32)\n",
      "      %12 : Long()\n",
      "      %13 : Float(32, 1, 3, 3)\n",
      "      %14 : Float(32)\n",
      "      %15 : Float(32)\n",
      "      %16 : Float(32)\n",
      "      %17 : Float(32)\n",
      "      %18 : Long()\n",
      "      %19 : Float(16, 32, 1, 1)\n",
      "      %20 : Float(16)\n",
      "      %21 : Float(16)\n",
      "      %22 : Float(16)\n",
      "      %23 : Float(16)\n",
      "      %24 : Long()\n",
      "      %25 : Float(16, 32, 1, 1)\n",
      "      %26 : Float(16)\n",
      "      %27 : Float(16)\n",
      "      %28 : Float(16)\n",
      "      %29 : Float(16)\n",
      "      %30 : Long()\n",
      "      %31 : Float(96, 16, 1, 1)\n",
      "      %32 : Float(96)\n",
      "      %33 : Float(96)\n",
      "      %34 : Float(96)\n",
      "      %35 : Float(96)\n",
      "      %36 : Long()\n",
      "      %37 : Float(96, 1, 3, 3)\n",
      "      %38 : Float(96)\n",
      "      %39 : Float(96)\n",
      "      %40 : Float(96)\n",
      "      %41 : Float(96)\n",
      "      %42 : Long()\n",
      "      %43 : Float(24, 96, 1, 1)\n",
      "      %44 : Float(24)\n",
      "      %45 : Float(24)\n",
      "      %46 : Float(24)\n",
      "      %47 : Float(24)\n",
      "      %48 : Long()\n",
      "      %49 : Float(24, 16, 1, 1)\n",
      "      %50 : Float(24)\n",
      "      %51 : Float(24)\n",
      "      %52 : Float(24)\n",
      "      %53 : Float(24)\n",
      "      %54 : Long()\n",
      "      %55 : Float(144, 24, 1, 1)\n",
      "      %56 : Float(144)\n",
      "      %57 : Float(144)\n",
      "      %58 : Float(144)\n",
      "      %59 : Float(144)\n",
      "      %60 : Long()\n",
      "      %61 : Float(144, 1, 3, 3)\n",
      "      %62 : Float(144)\n",
      "      %63 : Float(144)\n",
      "      %64 : Float(144)\n",
      "      %65 : Float(144)\n",
      "      %66 : Long()\n",
      "      %67 : Float(24, 144, 1, 1)\n",
      "      %68 : Float(24)\n",
      "      %69 : Float(24)\n",
      "      %70 : Float(24)\n",
      "      %71 : Float(24)\n",
      "      %72 : Long()\n",
      "      %73 : Float(144, 24, 1, 1)\n",
      "      %74 : Float(144)\n",
      "      %75 : Float(144)\n",
      "      %76 : Float(144)\n",
      "      %77 : Float(144)\n",
      "      %78 : Long()\n",
      "      %79 : Float(144, 1, 3, 3)\n",
      "      %80 : Float(144)\n",
      "      %81 : Float(144)\n",
      "      %82 : Float(144)\n",
      "      %83 : Float(144)\n",
      "      %84 : Long()\n",
      "      %85 : Float(32, 144, 1, 1)\n",
      "      %86 : Float(32)\n",
      "      %87 : Float(32)\n",
      "      %88 : Float(32)\n",
      "      %89 : Float(32)\n",
      "      %90 : Long()\n",
      "      %91 : Float(192, 32, 1, 1)\n",
      "      %92 : Float(192)\n",
      "      %93 : Float(192)\n",
      "      %94 : Float(192)\n",
      "      %95 : Float(192)\n",
      "      %96 : Long()\n",
      "      %97 : Float(192, 1, 3, 3)\n",
      "      %98 : Float(192)\n",
      "      %99 : Float(192)\n",
      "      %100 : Float(192)\n",
      "      %101 : Float(192)\n",
      "      %102 : Long()\n",
      "      %103 : Float(32, 192, 1, 1)\n",
      "      %104 : Float(32)\n",
      "      %105 : Float(32)\n",
      "      %106 : Float(32)\n",
      "      %107 : Float(32)\n",
      "      %108 : Long()\n",
      "      %109 : Float(192, 32, 1, 1)\n",
      "      %110 : Float(192)\n",
      "      %111 : Float(192)\n",
      "      %112 : Float(192)\n",
      "      %113 : Float(192)\n",
      "      %114 : Long()\n",
      "      %115 : Float(192, 1, 3, 3)\n",
      "      %116 : Float(192)\n",
      "      %117 : Float(192)\n",
      "      %118 : Float(192)\n",
      "      %119 : Float(192)\n",
      "      %120 : Long()\n",
      "      %121 : Float(32, 192, 1, 1)\n",
      "      %122 : Float(32)\n",
      "      %123 : Float(32)\n",
      "      %124 : Float(32)\n",
      "      %125 : Float(32)\n",
      "      %126 : Long()\n",
      "      %127 : Float(192, 32, 1, 1)\n",
      "      %128 : Float(192)\n",
      "      %129 : Float(192)\n",
      "      %130 : Float(192)\n",
      "      %131 : Float(192)\n",
      "      %132 : Long()\n",
      "      %133 : Float(192, 1, 3, 3)\n",
      "      %134 : Float(192)\n",
      "      %135 : Float(192)\n",
      "      %136 : Float(192)\n",
      "      %137 : Float(192)\n",
      "      %138 : Long()\n",
      "      %139 : Float(64, 192, 1, 1)\n",
      "      %140 : Float(64)\n",
      "      %141 : Float(64)\n",
      "      %142 : Float(64)\n",
      "      %143 : Float(64)\n",
      "      %144 : Long()\n",
      "      %145 : Float(384, 64, 1, 1)\n",
      "      %146 : Float(384)\n",
      "      %147 : Float(384)\n",
      "      %148 : Float(384)\n",
      "      %149 : Float(384)\n",
      "      %150 : Long()\n",
      "      %151 : Float(384, 1, 3, 3)\n",
      "      %152 : Float(384)\n",
      "      %153 : Float(384)\n",
      "      %154 : Float(384)\n",
      "      %155 : Float(384)\n",
      "      %156 : Long()\n",
      "      %157 : Float(64, 384, 1, 1)\n",
      "      %158 : Float(64)\n",
      "      %159 : Float(64)\n",
      "      %160 : Float(64)\n",
      "      %161 : Float(64)\n",
      "      %162 : Long()\n",
      "      %163 : Float(384, 64, 1, 1)\n",
      "      %164 : Float(384)\n",
      "      %165 : Float(384)\n",
      "      %166 : Float(384)\n",
      "      %167 : Float(384)\n",
      "      %168 : Long()\n",
      "      %169 : Float(384, 1, 3, 3)\n",
      "      %170 : Float(384)\n",
      "      %171 : Float(384)\n",
      "      %172 : Float(384)\n",
      "      %173 : Float(384)\n",
      "      %174 : Long()\n",
      "      %175 : Float(64, 384, 1, 1)\n",
      "      %176 : Float(64)\n",
      "      %177 : Float(64)\n",
      "      %178 : Float(64)\n",
      "      %179 : Float(64)\n",
      "      %180 : Long()\n",
      "      %181 : Float(384, 64, 1, 1)\n",
      "      %182 : Float(384)\n",
      "      %183 : Float(384)\n",
      "      %184 : Float(384)\n",
      "      %185 : Float(384)\n",
      "      %186 : Long()\n",
      "      %187 : Float(384, 1, 3, 3)\n",
      "      %188 : Float(384)\n",
      "      %189 : Float(384)\n",
      "      %190 : Float(384)\n",
      "      %191 : Float(384)\n",
      "      %192 : Long()\n",
      "      %193 : Float(64, 384, 1, 1)\n",
      "      %194 : Float(64)\n",
      "      %195 : Float(64)\n",
      "      %196 : Float(64)\n",
      "      %197 : Float(64)\n",
      "      %198 : Long()\n",
      "      %199 : Float(384, 64, 1, 1)\n",
      "      %200 : Float(384)\n",
      "      %201 : Float(384)\n",
      "      %202 : Float(384)\n",
      "      %203 : Float(384)\n",
      "      %204 : Long()\n",
      "      %205 : Float(384, 1, 3, 3)\n",
      "      %206 : Float(384)\n",
      "      %207 : Float(384)\n",
      "      %208 : Float(384)\n",
      "      %209 : Float(384)\n",
      "      %210 : Long()\n",
      "      %211 : Float(96, 384, 1, 1)\n",
      "      %212 : Float(96)\n",
      "      %213 : Float(96)\n",
      "      %214 : Float(96)\n",
      "      %215 : Float(96)\n",
      "      %216 : Long()\n",
      "      %217 : Float(96, 64, 1, 1)\n",
      "      %218 : Float(96)\n",
      "      %219 : Float(96)\n",
      "      %220 : Float(96)\n",
      "      %221 : Float(96)\n",
      "      %222 : Long()\n",
      "      %223 : Float(576, 96, 1, 1)\n",
      "      %224 : Float(576)\n",
      "      %225 : Float(576)\n",
      "      %226 : Float(576)\n",
      "      %227 : Float(576)\n",
      "      %228 : Long()\n",
      "      %229 : Float(576, 1, 3, 3)\n",
      "      %230 : Float(576)\n",
      "      %231 : Float(576)\n",
      "      %232 : Float(576)\n",
      "      %233 : Float(576)\n",
      "      %234 : Long()\n",
      "      %235 : Float(96, 576, 1, 1)\n",
      "      %236 : Float(96)\n",
      "      %237 : Float(96)\n",
      "      %238 : Float(96)\n",
      "      %239 : Float(96)\n",
      "      %240 : Long()\n",
      "      %241 : Float(576, 96, 1, 1)\n",
      "      %242 : Float(576)\n",
      "      %243 : Float(576)\n",
      "      %244 : Float(576)\n",
      "      %245 : Float(576)\n",
      "      %246 : Long()\n",
      "      %247 : Float(576, 1, 3, 3)\n",
      "      %248 : Float(576)\n",
      "      %249 : Float(576)\n",
      "      %250 : Float(576)\n",
      "      %251 : Float(576)\n",
      "      %252 : Long()\n",
      "      %253 : Float(96, 576, 1, 1)\n",
      "      %254 : Float(96)\n",
      "      %255 : Float(96)\n",
      "      %256 : Float(96)\n",
      "      %257 : Float(96)\n",
      "      %258 : Long()\n",
      "      %259 : Float(576, 96, 1, 1)\n",
      "      %260 : Float(576)\n",
      "      %261 : Float(576)\n",
      "      %262 : Float(576)\n",
      "      %263 : Float(576)\n",
      "      %264 : Long()\n",
      "      %265 : Float(576, 1, 3, 3)\n",
      "      %266 : Float(576)\n",
      "      %267 : Float(576)\n",
      "      %268 : Float(576)\n",
      "      %269 : Float(576)\n",
      "      %270 : Long()\n",
      "      %271 : Float(160, 576, 1, 1)\n",
      "      %272 : Float(160)\n",
      "      %273 : Float(160)\n",
      "      %274 : Float(160)\n",
      "      %275 : Float(160)\n",
      "      %276 : Long()\n",
      "      %277 : Float(960, 160, 1, 1)\n",
      "      %278 : Float(960)\n",
      "      %279 : Float(960)\n",
      "      %280 : Float(960)\n",
      "      %281 : Float(960)\n",
      "      %282 : Long()\n",
      "      %283 : Float(960, 1, 3, 3)\n",
      "      %284 : Float(960)\n",
      "      %285 : Float(960)\n",
      "      %286 : Float(960)\n",
      "      %287 : Float(960)\n",
      "      %288 : Long()\n",
      "      %289 : Float(160, 960, 1, 1)\n",
      "      %290 : Float(160)\n",
      "      %291 : Float(160)\n",
      "      %292 : Float(160)\n",
      "      %293 : Float(160)\n",
      "      %294 : Long()\n",
      "      %295 : Float(960, 160, 1, 1)\n",
      "      %296 : Float(960)\n",
      "      %297 : Float(960)\n",
      "      %298 : Float(960)\n",
      "      %299 : Float(960)\n",
      "      %300 : Long()\n",
      "      %301 : Float(960, 1, 3, 3)\n",
      "      %302 : Float(960)\n",
      "      %303 : Float(960)\n",
      "      %304 : Float(960)\n",
      "      %305 : Float(960)\n",
      "      %306 : Long()\n",
      "      %307 : Float(160, 960, 1, 1)\n",
      "      %308 : Float(160)\n",
      "      %309 : Float(160)\n",
      "      %310 : Float(160)\n",
      "      %311 : Float(160)\n",
      "      %312 : Long()\n",
      "      %313 : Float(960, 160, 1, 1)\n",
      "      %314 : Float(960)\n",
      "      %315 : Float(960)\n",
      "      %316 : Float(960)\n",
      "      %317 : Float(960)\n",
      "      %318 : Long()\n",
      "      %319 : Float(960, 1, 3, 3)\n",
      "      %320 : Float(960)\n",
      "      %321 : Float(960)\n",
      "      %322 : Float(960)\n",
      "      %323 : Float(960)\n",
      "      %324 : Long()\n",
      "      %325 : Float(320, 960, 1, 1)\n",
      "      %326 : Float(320)\n",
      "      %327 : Float(320)\n",
      "      %328 : Float(320)\n",
      "      %329 : Float(320)\n",
      "      %330 : Long()\n",
      "      %331 : Float(320, 160, 1, 1)\n",
      "      %332 : Float(320)\n",
      "      %333 : Float(320)\n",
      "      %334 : Float(320)\n",
      "      %335 : Float(320)\n",
      "      %336 : Long()\n",
      "      %337 : Float(1280, 320, 1, 1)\n",
      "      %338 : Float(1280)\n",
      "      %339 : Float(1280)\n",
      "      %340 : Float(1280)\n",
      "      %341 : Float(1280)\n",
      "      %342 : Long()\n",
      "      %343 : Float(10, 1280)\n",
      "      %344 : Float(10)) {\n",
      "  %345 : Float(1, 3, 32, 32) = ^Scatter([0], None, 0)(%0), scope: DataParallel\n",
      "  %346 : Float(1, 32, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%345, %1), scope: DataParallel/MobileNetV2[module]/Conv2d[conv1]\n",
      "  %347 : Float(1, 32, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%346, %2, %3, %4, %5), scope: DataParallel/MobileNetV2[module]/BatchNorm2d[bn1]\n",
      "  %348 : Float(1, 32, 32, 32) = onnx::Relu(%347), scope: DataParallel/MobileNetV2[module]\n",
      "  %349 : Float(1, 32, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%348, %7), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Conv2d[conv1]\n",
      "  %350 : Float(1, 32, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%349, %8, %9, %10, %11), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/BatchNorm2d[bn1]\n",
      "  %351 : Float(1, 32, 32, 32) = onnx::Relu(%350), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]\n",
      "  %352 : Float(1, 32, 32, 32) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%351, %13), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Conv2d[conv2]\n",
      "  %353 : Float(1, 32, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%352, %14, %15, %16, %17), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/BatchNorm2d[bn2]\n",
      "  %354 : Float(1, 32, 32, 32) = onnx::Relu(%353), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]\n",
      "  %355 : Float(1, 16, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%354, %19), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Conv2d[conv3]\n",
      "  %356 : Float(1, 16, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%355, %20, %21, %22, %23), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/BatchNorm2d[bn3]\n",
      "  %357 : Float(1, 16, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%348, %25), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Sequential[shortcut]/Conv2d[0]\n",
      "  %358 : Float(1, 16, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%357, %26, %27, %28, %29), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Sequential[shortcut]/BatchNorm2d[1]\n",
      "  %359 : Float(1, 16, 32, 32) = onnx::Add(%356, %358), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]\n",
      "  %360 : Float(1, 96, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%359, %31), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Conv2d[conv1]\n",
      "  %361 : Float(1, 96, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%360, %32, %33, %34, %35), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/BatchNorm2d[bn1]\n",
      "  %362 : Float(1, 96, 32, 32) = onnx::Relu(%361), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]\n",
      "  %363 : Float(1, 96, 32, 32) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%362, %37), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Conv2d[conv2]\n",
      "  %364 : Float(1, 96, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%363, %38, %39, %40, %41), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/BatchNorm2d[bn2]\n",
      "  %365 : Float(1, 96, 32, 32) = onnx::Relu(%364), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]\n",
      "  %366 : Float(1, 24, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%365, %43), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Conv2d[conv3]\n",
      "  %367 : Float(1, 24, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%366, %44, %45, %46, %47), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/BatchNorm2d[bn3]\n",
      "  %368 : Float(1, 24, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%359, %49), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Sequential[shortcut]/Conv2d[0]\n",
      "  %369 : Float(1, 24, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%368, %50, %51, %52, %53), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Sequential[shortcut]/BatchNorm2d[1]\n",
      "  %370 : Float(1, 24, 32, 32) = onnx::Add(%367, %369), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]\n",
      "  %371 : Float(1, 144, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%370, %55), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/Conv2d[conv1]\n",
      "  %372 : Float(1, 144, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%371, %56, %57, %58, %59), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/BatchNorm2d[bn1]\n",
      "  %373 : Float(1, 144, 32, 32) = onnx::Relu(%372), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]\n",
      "  %374 : Float(1, 144, 32, 32) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%373, %61), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/Conv2d[conv2]\n",
      "  %375 : Float(1, 144, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%374, %62, %63, %64, %65), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/BatchNorm2d[bn2]\n",
      "  %376 : Float(1, 144, 32, 32) = onnx::Relu(%375), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]\n",
      "  %377 : Float(1, 24, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%376, %67), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/Conv2d[conv3]\n",
      "  %378 : Float(1, 24, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%377, %68, %69, %70, %71), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/BatchNorm2d[bn3]\n",
      "  %379 : Float(1, 24, 32, 32) = onnx::Add(%378, %370), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]\n",
      "  %380 : Float(1, 144, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%379, %73), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/Conv2d[conv1]\n",
      "  %381 : Float(1, 144, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%380, %74, %75, %76, %77), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/BatchNorm2d[bn1]\n",
      "  %382 : Float(1, 144, 32, 32) = onnx::Relu(%381), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]\n",
      "  %383 : Float(1, 144, 16, 16) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%382, %79), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/Conv2d[conv2]\n",
      "  %384 : Float(1, 144, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%383, %80, %81, %82, %83), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/BatchNorm2d[bn2]\n",
      "  %385 : Float(1, 144, 16, 16) = onnx::Relu(%384), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]\n",
      "  %386 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%385, %85), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/Conv2d[conv3]\n",
      "  %387 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%386, %86, %87, %88, %89), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/BatchNorm2d[bn3]\n",
      "  %388 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%387, %91), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/Conv2d[conv1]\n",
      "  %389 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%388, %92, %93, %94, %95), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/BatchNorm2d[bn1]\n",
      "  %390 : Float(1, 192, 16, 16) = onnx::Relu(%389), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]\n",
      "  %391 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%390, %97), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/Conv2d[conv2]\n",
      "  %392 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%391, %98, %99, %100, %101), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/BatchNorm2d[bn2]\n",
      "  %393 : Float(1, 192, 16, 16) = onnx::Relu(%392), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]\n",
      "  %394 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%393, %103), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/Conv2d[conv3]\n",
      "  %395 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%394, %104, %105, %106, %107), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/BatchNorm2d[bn3]\n",
      "  %396 : Float(1, 32, 16, 16) = onnx::Add(%395, %387), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]\n",
      "  %397 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%396, %109), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/Conv2d[conv1]\n",
      "  %398 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%397, %110, %111, %112, %113), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/BatchNorm2d[bn1]\n",
      "  %399 : Float(1, 192, 16, 16) = onnx::Relu(%398), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]\n",
      "  %400 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%399, %115), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/Conv2d[conv2]\n",
      "  %401 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%400, %116, %117, %118, %119), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/BatchNorm2d[bn2]\n",
      "  %402 : Float(1, 192, 16, 16) = onnx::Relu(%401), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]\n",
      "  %403 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%402, %121), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/Conv2d[conv3]\n",
      "  %404 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%403, %122, %123, %124, %125), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/BatchNorm2d[bn3]\n",
      "  %405 : Float(1, 32, 16, 16) = onnx::Add(%404, %396), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]\n",
      "  %406 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%405, %127), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/Conv2d[conv1]\n",
      "  %407 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%406, %128, %129, %130, %131), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/BatchNorm2d[bn1]\n",
      "  %408 : Float(1, 192, 16, 16) = onnx::Relu(%407), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]\n",
      "  %409 : Float(1, 192, 8, 8) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%408, %133), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/Conv2d[conv2]\n",
      "  %410 : Float(1, 192, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%409, %134, %135, %136, %137), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/BatchNorm2d[bn2]\n",
      "  %411 : Float(1, 192, 8, 8) = onnx::Relu(%410), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]\n",
      "  %412 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%411, %139), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/Conv2d[conv3]\n",
      "  %413 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%412, %140, %141, %142, %143), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/BatchNorm2d[bn3]\n",
      "  %414 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%413, %145), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/Conv2d[conv1]\n",
      "  %415 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%414, %146, %147, %148, %149), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/BatchNorm2d[bn1]\n",
      "  %416 : Float(1, 384, 8, 8) = onnx::Relu(%415), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]\n",
      "  %417 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%416, %151), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/Conv2d[conv2]\n",
      "  %418 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%417, %152, %153, %154, %155), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/BatchNorm2d[bn2]\n",
      "  %419 : Float(1, 384, 8, 8) = onnx::Relu(%418), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]\n",
      "  %420 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%419, %157), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/Conv2d[conv3]\n",
      "  %421 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%420, %158, %159, %160, %161), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/BatchNorm2d[bn3]\n",
      "  %422 : Float(1, 64, 8, 8) = onnx::Add(%421, %413), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]\n",
      "  %423 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%422, %163), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/Conv2d[conv1]\n",
      "  %424 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%423, %164, %165, %166, %167), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/BatchNorm2d[bn1]\n",
      "  %425 : Float(1, 384, 8, 8) = onnx::Relu(%424), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]\n",
      "  %426 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%425, %169), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/Conv2d[conv2]\n",
      "  %427 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%426, %170, %171, %172, %173), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/BatchNorm2d[bn2]\n",
      "  %428 : Float(1, 384, 8, 8) = onnx::Relu(%427), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]\n",
      "  %429 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%428, %175), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/Conv2d[conv3]\n",
      "  %430 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%429, %176, %177, %178, %179), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/BatchNorm2d[bn3]\n",
      "  %431 : Float(1, 64, 8, 8) = onnx::Add(%430, %422), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]\n",
      "  %432 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%431, %181), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/Conv2d[conv1]\n",
      "  %433 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%432, %182, %183, %184, %185), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/BatchNorm2d[bn1]\n",
      "  %434 : Float(1, 384, 8, 8) = onnx::Relu(%433), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]\n",
      "  %435 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%434, %187), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/Conv2d[conv2]\n",
      "  %436 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%435, %188, %189, %190, %191), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/BatchNorm2d[bn2]\n",
      "  %437 : Float(1, 384, 8, 8) = onnx::Relu(%436), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]\n",
      "  %438 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%437, %193), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/Conv2d[conv3]\n",
      "  %439 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%438, %194, %195, %196, %197), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/BatchNorm2d[bn3]\n",
      "  %440 : Float(1, 64, 8, 8) = onnx::Add(%439, %431), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]\n",
      "  %441 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%440, %199), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Conv2d[conv1]\n",
      "  %442 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%441, %200, %201, %202, %203), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/BatchNorm2d[bn1]\n",
      "  %443 : Float(1, 384, 8, 8) = onnx::Relu(%442), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]\n",
      "  %444 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%443, %205), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Conv2d[conv2]\n",
      "  %445 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%444, %206, %207, %208, %209), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/BatchNorm2d[bn2]\n",
      "  %446 : Float(1, 384, 8, 8) = onnx::Relu(%445), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]\n",
      "  %447 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%446, %211), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Conv2d[conv3]\n",
      "  %448 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%447, %212, %213, %214, %215), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/BatchNorm2d[bn3]\n",
      "  %449 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%440, %217), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Sequential[shortcut]/Conv2d[0]\n",
      "  %450 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%449, %218, %219, %220, %221), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Sequential[shortcut]/BatchNorm2d[1]\n",
      "  %451 : Float(1, 96, 8, 8) = onnx::Add(%448, %450), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]\n",
      "  %452 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%451, %223), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/Conv2d[conv1]\n",
      "  %453 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%452, %224, %225, %226, %227), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/BatchNorm2d[bn1]\n",
      "  %454 : Float(1, 576, 8, 8) = onnx::Relu(%453), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]\n",
      "  %455 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%454, %229), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/Conv2d[conv2]\n",
      "  %456 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%455, %230, %231, %232, %233), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/BatchNorm2d[bn2]\n",
      "  %457 : Float(1, 576, 8, 8) = onnx::Relu(%456), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]\n",
      "  %458 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%457, %235), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/Conv2d[conv3]\n",
      "  %459 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%458, %236, %237, %238, %239), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/BatchNorm2d[bn3]\n",
      "  %460 : Float(1, 96, 8, 8) = onnx::Add(%459, %451), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]\n",
      "  %461 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%460, %241), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/Conv2d[conv1]\n",
      "  %462 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%461, %242, %243, %244, %245), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/BatchNorm2d[bn1]\n",
      "  %463 : Float(1, 576, 8, 8) = onnx::Relu(%462), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]\n",
      "  %464 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%463, %247), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/Conv2d[conv2]\n",
      "  %465 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%464, %248, %249, %250, %251), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/BatchNorm2d[bn2]\n",
      "  %466 : Float(1, 576, 8, 8) = onnx::Relu(%465), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]\n",
      "  %467 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%466, %253), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/Conv2d[conv3]\n",
      "  %468 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%467, %254, %255, %256, %257), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/BatchNorm2d[bn3]\n",
      "  %469 : Float(1, 96, 8, 8) = onnx::Add(%468, %460), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]\n",
      "  %470 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%469, %259), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/Conv2d[conv1]\n",
      "  %471 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%470, %260, %261, %262, %263), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/BatchNorm2d[bn1]\n",
      "  %472 : Float(1, 576, 8, 8) = onnx::Relu(%471), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]\n",
      "  %473 : Float(1, 576, 4, 4) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%472, %265), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/Conv2d[conv2]\n",
      "  %474 : Float(1, 576, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%473, %266, %267, %268, %269), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/BatchNorm2d[bn2]\n",
      "  %475 : Float(1, 576, 4, 4) = onnx::Relu(%474), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]\n",
      "  %476 : Float(1, 160, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%475, %271), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/Conv2d[conv3]\n",
      "  %477 : Float(1, 160, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%476, %272, %273, %274, %275), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/BatchNorm2d[bn3]\n",
      "  %478 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%477, %277), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/Conv2d[conv1]\n",
      "  %479 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%478, %278, %279, %280, %281), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/BatchNorm2d[bn1]\n",
      "  %480 : Float(1, 960, 4, 4) = onnx::Relu(%479), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]\n",
      "  %481 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%480, %283), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/Conv2d[conv2]\n",
      "  %482 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%481, %284, %285, %286, %287), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/BatchNorm2d[bn2]\n",
      "  %483 : Float(1, 960, 4, 4) = onnx::Relu(%482), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]\n",
      "  %484 : Float(1, 160, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%483, %289), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/Conv2d[conv3]\n",
      "  %485 : Float(1, 160, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%484, %290, %291, %292, %293), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/BatchNorm2d[bn3]\n",
      "  %486 : Float(1, 160, 4, 4) = onnx::Add(%485, %477), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]\n",
      "  %487 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%486, %295), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/Conv2d[conv1]\n",
      "  %488 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%487, %296, %297, %298, %299), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/BatchNorm2d[bn1]\n",
      "  %489 : Float(1, 960, 4, 4) = onnx::Relu(%488), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]\n",
      "  %490 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%489, %301), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/Conv2d[conv2]\n",
      "  %491 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%490, %302, %303, %304, %305), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/BatchNorm2d[bn2]\n",
      "  %492 : Float(1, 960, 4, 4) = onnx::Relu(%491), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]\n",
      "  %493 : Float(1, 160, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%492, %307), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/Conv2d[conv3]\n",
      "  %494 : Float(1, 160, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%493, %308, %309, %310, %311), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/BatchNorm2d[bn3]\n",
      "  %495 : Float(1, 160, 4, 4) = onnx::Add(%494, %486), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]\n",
      "  %496 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%495, %313), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Conv2d[conv1]\n",
      "  %497 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%496, %314, %315, %316, %317), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/BatchNorm2d[bn1]\n",
      "  %498 : Float(1, 960, 4, 4) = onnx::Relu(%497), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]\n",
      "  %499 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%498, %319), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Conv2d[conv2]\n",
      "  %500 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%499, %320, %321, %322, %323), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/BatchNorm2d[bn2]\n",
      "  %501 : Float(1, 960, 4, 4) = onnx::Relu(%500), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]\n",
      "  %502 : Float(1, 320, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%501, %325), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Conv2d[conv3]\n",
      "  %503 : Float(1, 320, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%502, %326, %327, %328, %329), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/BatchNorm2d[bn3]\n",
      "  %504 : Float(1, 320, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%495, %331), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Sequential[shortcut]/Conv2d[0]\n",
      "  %505 : Float(1, 320, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%504, %332, %333, %334, %335), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Sequential[shortcut]/BatchNorm2d[1]\n",
      "  %506 : Float(1, 320, 4, 4) = onnx::Add(%503, %505), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]\n",
      "  %507 : Float(1, 1280, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%506, %337), scope: DataParallel/MobileNetV2[module]/Conv2d[conv2]\n",
      "  %508 : Float(1, 1280, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%507, %338, %339, %340, %341), scope: DataParallel/MobileNetV2[module]/BatchNorm2d[bn2]\n",
      "  %509 : Float(1, 1280, 4, 4) = onnx::Relu(%508), scope: DataParallel/MobileNetV2[module]\n",
      "  %510 : Dynamic = onnx::Pad[mode=constant, pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0](%509), scope: DataParallel/MobileNetV2[module]\n",
      "  %511 : Float(1, 1280, 1, 1) = onnx::AveragePool[kernel_shape=[4, 4], pads=[0, 0, 0, 0], strides=[4, 4]](%510), scope: DataParallel/MobileNetV2[module]\n",
      "  %512 : Dynamic = onnx::Shape(%511), scope: DataParallel/MobileNetV2[module]\n",
      "  %513 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%512), scope: DataParallel/MobileNetV2[module]\n",
      "  %514 : Long() = onnx::Squeeze[axes=[0]](%513), scope: DataParallel/MobileNetV2[module]\n",
      "  %515 : Long() = onnx::Constant[value={-1}](), scope: DataParallel/MobileNetV2[module]\n",
      "  %516 : Dynamic = onnx::Unsqueeze[axes=[0]](%514), scope: DataParallel/MobileNetV2[module]\n",
      "  %517 : Dynamic = onnx::Unsqueeze[axes=[0]](%515), scope: DataParallel/MobileNetV2[module]\n",
      "  %518 : Dynamic = onnx::Concat[axis=0](%516, %517), scope: DataParallel/MobileNetV2[module]\n",
      "  %519 : Float(1, 1280) = onnx::Reshape(%511, %518), scope: DataParallel/MobileNetV2[module]\n",
      "  %520 : Float(1, 10) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%519, %343, %344), scope: DataParallel/MobileNetV2[module]/Linear[linear]\n",
      "  return (%520);\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ONNX export failed: Couldn't export Python operator Scatter\n\nDefined at:\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py(13): scatter_map\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py(15): scatter_map\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py(28): scatter\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py(35): scatter_kwargs\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py(130): scatter\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py(119): forward\n/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py(465): _slow_forward\n/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py(475): __call__\n/opt/conda/lib/python3.6/site-packages/torch/jit/__init__.py(109): forward\n/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py(477): __call__\n/opt/conda/lib/python3.6/site-packages/torch/jit/__init__.py(77): get_trace_graph\n/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py(144): _trace_and_get_graph_from_model\n/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py(177): _model_to_graph\n/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py(226): _export\n/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py(94): export\n/opt/conda/lib/python3.6/site-packages/torch/onnx/__init__.py(26): export\n<ipython-input-4-7a10c6f279fd>(47): <module>\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3267): run_code\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3191): run_ast_nodes\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3020): run_cell_async\n/opt/conda/lib/python3.6/site-packages/IPython/core/async_helpers.py(67): _pseudo_sync_runner\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2845): _run_cell\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2819): run_cell\n/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py(536): run_cell\n/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py(294): do_execute\n/opt/conda/lib/python3.6/site-packages/tornado/gen.py(326): wrapper\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py(534): execute_request\n/opt/conda/lib/python3.6/site-packages/tornado/gen.py(326): wrapper\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py(267): dispatch_shell\n/opt/conda/lib/python3.6/site-packages/tornado/gen.py(326): wrapper\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py(357): process_one\n/opt/conda/lib/python3.6/site-packages/tornado/gen.py(1147): run\n/opt/conda/lib/python3.6/site-packages/tornado/gen.py(1233): inner\n/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py(300): null_wrapper\n/opt/conda/lib/python3.6/site-packages/tornado/ioloop.py(758): _run_callback\n/opt/conda/lib/python3.6/asyncio/events.py(145): _run\n/opt/conda/lib/python3.6/asyncio/base_events.py(1434): _run_once\n/opt/conda/lib/python3.6/asyncio/base_events.py(422): run_forever\n/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py(132): start\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py(505): start\n/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py(658): launch_instance\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py(16): <module>\n/opt/conda/lib/python3.6/runpy.py(85): _run_code\n/opt/conda/lib/python3.6/runpy.py(193): _run_module_as_main\n\n\nGraph we tried to export:\ngraph(%0 : Float(1, 3, 32, 32)\n      %1 : Float(32, 3, 3, 3)\n      %2 : Float(32)\n      %3 : Float(32)\n      %4 : Float(32)\n      %5 : Float(32)\n      %6 : Long()\n      %7 : Float(32, 32, 1, 1)\n      %8 : Float(32)\n      %9 : Float(32)\n      %10 : Float(32)\n      %11 : Float(32)\n      %12 : Long()\n      %13 : Float(32, 1, 3, 3)\n      %14 : Float(32)\n      %15 : Float(32)\n      %16 : Float(32)\n      %17 : Float(32)\n      %18 : Long()\n      %19 : Float(16, 32, 1, 1)\n      %20 : Float(16)\n      %21 : Float(16)\n      %22 : Float(16)\n      %23 : Float(16)\n      %24 : Long()\n      %25 : Float(16, 32, 1, 1)\n      %26 : Float(16)\n      %27 : Float(16)\n      %28 : Float(16)\n      %29 : Float(16)\n      %30 : Long()\n      %31 : Float(96, 16, 1, 1)\n      %32 : Float(96)\n      %33 : Float(96)\n      %34 : Float(96)\n      %35 : Float(96)\n      %36 : Long()\n      %37 : Float(96, 1, 3, 3)\n      %38 : Float(96)\n      %39 : Float(96)\n      %40 : Float(96)\n      %41 : Float(96)\n      %42 : Long()\n      %43 : Float(24, 96, 1, 1)\n      %44 : Float(24)\n      %45 : Float(24)\n      %46 : Float(24)\n      %47 : Float(24)\n      %48 : Long()\n      %49 : Float(24, 16, 1, 1)\n      %50 : Float(24)\n      %51 : Float(24)\n      %52 : Float(24)\n      %53 : Float(24)\n      %54 : Long()\n      %55 : Float(144, 24, 1, 1)\n      %56 : Float(144)\n      %57 : Float(144)\n      %58 : Float(144)\n      %59 : Float(144)\n      %60 : Long()\n      %61 : Float(144, 1, 3, 3)\n      %62 : Float(144)\n      %63 : Float(144)\n      %64 : Float(144)\n      %65 : Float(144)\n      %66 : Long()\n      %67 : Float(24, 144, 1, 1)\n      %68 : Float(24)\n      %69 : Float(24)\n      %70 : Float(24)\n      %71 : Float(24)\n      %72 : Long()\n      %73 : Float(144, 24, 1, 1)\n      %74 : Float(144)\n      %75 : Float(144)\n      %76 : Float(144)\n      %77 : Float(144)\n      %78 : Long()\n      %79 : Float(144, 1, 3, 3)\n      %80 : Float(144)\n      %81 : Float(144)\n      %82 : Float(144)\n      %83 : Float(144)\n      %84 : Long()\n      %85 : Float(32, 144, 1, 1)\n      %86 : Float(32)\n      %87 : Float(32)\n      %88 : Float(32)\n      %89 : Float(32)\n      %90 : Long()\n      %91 : Float(192, 32, 1, 1)\n      %92 : Float(192)\n      %93 : Float(192)\n      %94 : Float(192)\n      %95 : Float(192)\n      %96 : Long()\n      %97 : Float(192, 1, 3, 3)\n      %98 : Float(192)\n      %99 : Float(192)\n      %100 : Float(192)\n      %101 : Float(192)\n      %102 : Long()\n      %103 : Float(32, 192, 1, 1)\n      %104 : Float(32)\n      %105 : Float(32)\n      %106 : Float(32)\n      %107 : Float(32)\n      %108 : Long()\n      %109 : Float(192, 32, 1, 1)\n      %110 : Float(192)\n      %111 : Float(192)\n      %112 : Float(192)\n      %113 : Float(192)\n      %114 : Long()\n      %115 : Float(192, 1, 3, 3)\n      %116 : Float(192)\n      %117 : Float(192)\n      %118 : Float(192)\n      %119 : Float(192)\n      %120 : Long()\n      %121 : Float(32, 192, 1, 1)\n      %122 : Float(32)\n      %123 : Float(32)\n      %124 : Float(32)\n      %125 : Float(32)\n      %126 : Long()\n      %127 : Float(192, 32, 1, 1)\n      %128 : Float(192)\n      %129 : Float(192)\n      %130 : Float(192)\n      %131 : Float(192)\n      %132 : Long()\n      %133 : Float(192, 1, 3, 3)\n      %134 : Float(192)\n      %135 : Float(192)\n      %136 : Float(192)\n      %137 : Float(192)\n      %138 : Long()\n      %139 : Float(64, 192, 1, 1)\n      %140 : Float(64)\n      %141 : Float(64)\n      %142 : Float(64)\n      %143 : Float(64)\n      %144 : Long()\n      %145 : Float(384, 64, 1, 1)\n      %146 : Float(384)\n      %147 : Float(384)\n      %148 : Float(384)\n      %149 : Float(384)\n      %150 : Long()\n      %151 : Float(384, 1, 3, 3)\n      %152 : Float(384)\n      %153 : Float(384)\n      %154 : Float(384)\n      %155 : Float(384)\n      %156 : Long()\n      %157 : Float(64, 384, 1, 1)\n      %158 : Float(64)\n      %159 : Float(64)\n      %160 : Float(64)\n      %161 : Float(64)\n      %162 : Long()\n      %163 : Float(384, 64, 1, 1)\n      %164 : Float(384)\n      %165 : Float(384)\n      %166 : Float(384)\n      %167 : Float(384)\n      %168 : Long()\n      %169 : Float(384, 1, 3, 3)\n      %170 : Float(384)\n      %171 : Float(384)\n      %172 : Float(384)\n      %173 : Float(384)\n      %174 : Long()\n      %175 : Float(64, 384, 1, 1)\n      %176 : Float(64)\n      %177 : Float(64)\n      %178 : Float(64)\n      %179 : Float(64)\n      %180 : Long()\n      %181 : Float(384, 64, 1, 1)\n      %182 : Float(384)\n      %183 : Float(384)\n      %184 : Float(384)\n      %185 : Float(384)\n      %186 : Long()\n      %187 : Float(384, 1, 3, 3)\n      %188 : Float(384)\n      %189 : Float(384)\n      %190 : Float(384)\n      %191 : Float(384)\n      %192 : Long()\n      %193 : Float(64, 384, 1, 1)\n      %194 : Float(64)\n      %195 : Float(64)\n      %196 : Float(64)\n      %197 : Float(64)\n      %198 : Long()\n      %199 : Float(384, 64, 1, 1)\n      %200 : Float(384)\n      %201 : Float(384)\n      %202 : Float(384)\n      %203 : Float(384)\n      %204 : Long()\n      %205 : Float(384, 1, 3, 3)\n      %206 : Float(384)\n      %207 : Float(384)\n      %208 : Float(384)\n      %209 : Float(384)\n      %210 : Long()\n      %211 : Float(96, 384, 1, 1)\n      %212 : Float(96)\n      %213 : Float(96)\n      %214 : Float(96)\n      %215 : Float(96)\n      %216 : Long()\n      %217 : Float(96, 64, 1, 1)\n      %218 : Float(96)\n      %219 : Float(96)\n      %220 : Float(96)\n      %221 : Float(96)\n      %222 : Long()\n      %223 : Float(576, 96, 1, 1)\n      %224 : Float(576)\n      %225 : Float(576)\n      %226 : Float(576)\n      %227 : Float(576)\n      %228 : Long()\n      %229 : Float(576, 1, 3, 3)\n      %230 : Float(576)\n      %231 : Float(576)\n      %232 : Float(576)\n      %233 : Float(576)\n      %234 : Long()\n      %235 : Float(96, 576, 1, 1)\n      %236 : Float(96)\n      %237 : Float(96)\n      %238 : Float(96)\n      %239 : Float(96)\n      %240 : Long()\n      %241 : Float(576, 96, 1, 1)\n      %242 : Float(576)\n      %243 : Float(576)\n      %244 : Float(576)\n      %245 : Float(576)\n      %246 : Long()\n      %247 : Float(576, 1, 3, 3)\n      %248 : Float(576)\n      %249 : Float(576)\n      %250 : Float(576)\n      %251 : Float(576)\n      %252 : Long()\n      %253 : Float(96, 576, 1, 1)\n      %254 : Float(96)\n      %255 : Float(96)\n      %256 : Float(96)\n      %257 : Float(96)\n      %258 : Long()\n      %259 : Float(576, 96, 1, 1)\n      %260 : Float(576)\n      %261 : Float(576)\n      %262 : Float(576)\n      %263 : Float(576)\n      %264 : Long()\n      %265 : Float(576, 1, 3, 3)\n      %266 : Float(576)\n      %267 : Float(576)\n      %268 : Float(576)\n      %269 : Float(576)\n      %270 : Long()\n      %271 : Float(160, 576, 1, 1)\n      %272 : Float(160)\n      %273 : Float(160)\n      %274 : Float(160)\n      %275 : Float(160)\n      %276 : Long()\n      %277 : Float(960, 160, 1, 1)\n      %278 : Float(960)\n      %279 : Float(960)\n      %280 : Float(960)\n      %281 : Float(960)\n      %282 : Long()\n      %283 : Float(960, 1, 3, 3)\n      %284 : Float(960)\n      %285 : Float(960)\n      %286 : Float(960)\n      %287 : Float(960)\n      %288 : Long()\n      %289 : Float(160, 960, 1, 1)\n      %290 : Float(160)\n      %291 : Float(160)\n      %292 : Float(160)\n      %293 : Float(160)\n      %294 : Long()\n      %295 : Float(960, 160, 1, 1)\n      %296 : Float(960)\n      %297 : Float(960)\n      %298 : Float(960)\n      %299 : Float(960)\n      %300 : Long()\n      %301 : Float(960, 1, 3, 3)\n      %302 : Float(960)\n      %303 : Float(960)\n      %304 : Float(960)\n      %305 : Float(960)\n      %306 : Long()\n      %307 : Float(160, 960, 1, 1)\n      %308 : Float(160)\n      %309 : Float(160)\n      %310 : Float(160)\n      %311 : Float(160)\n      %312 : Long()\n      %313 : Float(960, 160, 1, 1)\n      %314 : Float(960)\n      %315 : Float(960)\n      %316 : Float(960)\n      %317 : Float(960)\n      %318 : Long()\n      %319 : Float(960, 1, 3, 3)\n      %320 : Float(960)\n      %321 : Float(960)\n      %322 : Float(960)\n      %323 : Float(960)\n      %324 : Long()\n      %325 : Float(320, 960, 1, 1)\n      %326 : Float(320)\n      %327 : Float(320)\n      %328 : Float(320)\n      %329 : Float(320)\n      %330 : Long()\n      %331 : Float(320, 160, 1, 1)\n      %332 : Float(320)\n      %333 : Float(320)\n      %334 : Float(320)\n      %335 : Float(320)\n      %336 : Long()\n      %337 : Float(1280, 320, 1, 1)\n      %338 : Float(1280)\n      %339 : Float(1280)\n      %340 : Float(1280)\n      %341 : Float(1280)\n      %342 : Long()\n      %343 : Float(10, 1280)\n      %344 : Float(10)) {\n  %345 : Float(1, 3, 32, 32) = ^Scatter([0], None, 0)(%0), scope: DataParallel\n  %346 : Float(1, 32, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%345, %1), scope: DataParallel/MobileNetV2[module]/Conv2d[conv1]\n  %347 : Float(1, 32, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%346, %2, %3, %4, %5), scope: DataParallel/MobileNetV2[module]/BatchNorm2d[bn1]\n  %348 : Float(1, 32, 32, 32) = onnx::Relu(%347), scope: DataParallel/MobileNetV2[module]\n  %349 : Float(1, 32, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%348, %7), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Conv2d[conv1]\n  %350 : Float(1, 32, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%349, %8, %9, %10, %11), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/BatchNorm2d[bn1]\n  %351 : Float(1, 32, 32, 32) = onnx::Relu(%350), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]\n  %352 : Float(1, 32, 32, 32) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%351, %13), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Conv2d[conv2]\n  %353 : Float(1, 32, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%352, %14, %15, %16, %17), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/BatchNorm2d[bn2]\n  %354 : Float(1, 32, 32, 32) = onnx::Relu(%353), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]\n  %355 : Float(1, 16, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%354, %19), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Conv2d[conv3]\n  %356 : Float(1, 16, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%355, %20, %21, %22, %23), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/BatchNorm2d[bn3]\n  %357 : Float(1, 16, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%348, %25), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Sequential[shortcut]/Conv2d[0]\n  %358 : Float(1, 16, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%357, %26, %27, %28, %29), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Sequential[shortcut]/BatchNorm2d[1]\n  %359 : Float(1, 16, 32, 32) = onnx::Add(%356, %358), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]\n  %360 : Float(1, 96, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%359, %31), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Conv2d[conv1]\n  %361 : Float(1, 96, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%360, %32, %33, %34, %35), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/BatchNorm2d[bn1]\n  %362 : Float(1, 96, 32, 32) = onnx::Relu(%361), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]\n  %363 : Float(1, 96, 32, 32) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%362, %37), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Conv2d[conv2]\n  %364 : Float(1, 96, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%363, %38, %39, %40, %41), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/BatchNorm2d[bn2]\n  %365 : Float(1, 96, 32, 32) = onnx::Relu(%364), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]\n  %366 : Float(1, 24, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%365, %43), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Conv2d[conv3]\n  %367 : Float(1, 24, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%366, %44, %45, %46, %47), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/BatchNorm2d[bn3]\n  %368 : Float(1, 24, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%359, %49), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Sequential[shortcut]/Conv2d[0]\n  %369 : Float(1, 24, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%368, %50, %51, %52, %53), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Sequential[shortcut]/BatchNorm2d[1]\n  %370 : Float(1, 24, 32, 32) = onnx::Add(%367, %369), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]\n  %371 : Float(1, 144, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%370, %55), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/Conv2d[conv1]\n  %372 : Float(1, 144, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%371, %56, %57, %58, %59), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/BatchNorm2d[bn1]\n  %373 : Float(1, 144, 32, 32) = onnx::Relu(%372), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]\n  %374 : Float(1, 144, 32, 32) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%373, %61), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/Conv2d[conv2]\n  %375 : Float(1, 144, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%374, %62, %63, %64, %65), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/BatchNorm2d[bn2]\n  %376 : Float(1, 144, 32, 32) = onnx::Relu(%375), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]\n  %377 : Float(1, 24, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%376, %67), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/Conv2d[conv3]\n  %378 : Float(1, 24, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%377, %68, %69, %70, %71), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/BatchNorm2d[bn3]\n  %379 : Float(1, 24, 32, 32) = onnx::Add(%378, %370), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]\n  %380 : Float(1, 144, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%379, %73), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/Conv2d[conv1]\n  %381 : Float(1, 144, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%380, %74, %75, %76, %77), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/BatchNorm2d[bn1]\n  %382 : Float(1, 144, 32, 32) = onnx::Relu(%381), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]\n  %383 : Float(1, 144, 16, 16) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%382, %79), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/Conv2d[conv2]\n  %384 : Float(1, 144, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%383, %80, %81, %82, %83), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/BatchNorm2d[bn2]\n  %385 : Float(1, 144, 16, 16) = onnx::Relu(%384), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]\n  %386 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%385, %85), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/Conv2d[conv3]\n  %387 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%386, %86, %87, %88, %89), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/BatchNorm2d[bn3]\n  %388 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%387, %91), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/Conv2d[conv1]\n  %389 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%388, %92, %93, %94, %95), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/BatchNorm2d[bn1]\n  %390 : Float(1, 192, 16, 16) = onnx::Relu(%389), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]\n  %391 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%390, %97), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/Conv2d[conv2]\n  %392 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%391, %98, %99, %100, %101), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/BatchNorm2d[bn2]\n  %393 : Float(1, 192, 16, 16) = onnx::Relu(%392), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]\n  %394 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%393, %103), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/Conv2d[conv3]\n  %395 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%394, %104, %105, %106, %107), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/BatchNorm2d[bn3]\n  %396 : Float(1, 32, 16, 16) = onnx::Add(%395, %387), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]\n  %397 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%396, %109), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/Conv2d[conv1]\n  %398 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%397, %110, %111, %112, %113), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/BatchNorm2d[bn1]\n  %399 : Float(1, 192, 16, 16) = onnx::Relu(%398), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]\n  %400 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%399, %115), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/Conv2d[conv2]\n  %401 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%400, %116, %117, %118, %119), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/BatchNorm2d[bn2]\n  %402 : Float(1, 192, 16, 16) = onnx::Relu(%401), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]\n  %403 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%402, %121), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/Conv2d[conv3]\n  %404 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%403, %122, %123, %124, %125), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/BatchNorm2d[bn3]\n  %405 : Float(1, 32, 16, 16) = onnx::Add(%404, %396), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]\n  %406 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%405, %127), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/Conv2d[conv1]\n  %407 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%406, %128, %129, %130, %131), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/BatchNorm2d[bn1]\n  %408 : Float(1, 192, 16, 16) = onnx::Relu(%407), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]\n  %409 : Float(1, 192, 8, 8) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%408, %133), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/Conv2d[conv2]\n  %410 : Float(1, 192, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%409, %134, %135, %136, %137), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/BatchNorm2d[bn2]\n  %411 : Float(1, 192, 8, 8) = onnx::Relu(%410), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]\n  %412 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%411, %139), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/Conv2d[conv3]\n  %413 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%412, %140, %141, %142, %143), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/BatchNorm2d[bn3]\n  %414 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%413, %145), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/Conv2d[conv1]\n  %415 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%414, %146, %147, %148, %149), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/BatchNorm2d[bn1]\n  %416 : Float(1, 384, 8, 8) = onnx::Relu(%415), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]\n  %417 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%416, %151), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/Conv2d[conv2]\n  %418 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%417, %152, %153, %154, %155), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/BatchNorm2d[bn2]\n  %419 : Float(1, 384, 8, 8) = onnx::Relu(%418), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]\n  %420 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%419, %157), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/Conv2d[conv3]\n  %421 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%420, %158, %159, %160, %161), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/BatchNorm2d[bn3]\n  %422 : Float(1, 64, 8, 8) = onnx::Add(%421, %413), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]\n  %423 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%422, %163), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/Conv2d[conv1]\n  %424 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%423, %164, %165, %166, %167), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/BatchNorm2d[bn1]\n  %425 : Float(1, 384, 8, 8) = onnx::Relu(%424), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]\n  %426 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%425, %169), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/Conv2d[conv2]\n  %427 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%426, %170, %171, %172, %173), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/BatchNorm2d[bn2]\n  %428 : Float(1, 384, 8, 8) = onnx::Relu(%427), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]\n  %429 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%428, %175), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/Conv2d[conv3]\n  %430 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%429, %176, %177, %178, %179), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/BatchNorm2d[bn3]\n  %431 : Float(1, 64, 8, 8) = onnx::Add(%430, %422), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]\n  %432 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%431, %181), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/Conv2d[conv1]\n  %433 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%432, %182, %183, %184, %185), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/BatchNorm2d[bn1]\n  %434 : Float(1, 384, 8, 8) = onnx::Relu(%433), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]\n  %435 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%434, %187), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/Conv2d[conv2]\n  %436 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%435, %188, %189, %190, %191), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/BatchNorm2d[bn2]\n  %437 : Float(1, 384, 8, 8) = onnx::Relu(%436), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]\n  %438 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%437, %193), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/Conv2d[conv3]\n  %439 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%438, %194, %195, %196, %197), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/BatchNorm2d[bn3]\n  %440 : Float(1, 64, 8, 8) = onnx::Add(%439, %431), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]\n  %441 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%440, %199), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Conv2d[conv1]\n  %442 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%441, %200, %201, %202, %203), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/BatchNorm2d[bn1]\n  %443 : Float(1, 384, 8, 8) = onnx::Relu(%442), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]\n  %444 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%443, %205), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Conv2d[conv2]\n  %445 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%444, %206, %207, %208, %209), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/BatchNorm2d[bn2]\n  %446 : Float(1, 384, 8, 8) = onnx::Relu(%445), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]\n  %447 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%446, %211), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Conv2d[conv3]\n  %448 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%447, %212, %213, %214, %215), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/BatchNorm2d[bn3]\n  %449 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%440, %217), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Sequential[shortcut]/Conv2d[0]\n  %450 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%449, %218, %219, %220, %221), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Sequential[shortcut]/BatchNorm2d[1]\n  %451 : Float(1, 96, 8, 8) = onnx::Add(%448, %450), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]\n  %452 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%451, %223), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/Conv2d[conv1]\n  %453 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%452, %224, %225, %226, %227), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/BatchNorm2d[bn1]\n  %454 : Float(1, 576, 8, 8) = onnx::Relu(%453), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]\n  %455 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%454, %229), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/Conv2d[conv2]\n  %456 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%455, %230, %231, %232, %233), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/BatchNorm2d[bn2]\n  %457 : Float(1, 576, 8, 8) = onnx::Relu(%456), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]\n  %458 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%457, %235), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/Conv2d[conv3]\n  %459 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%458, %236, %237, %238, %239), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/BatchNorm2d[bn3]\n  %460 : Float(1, 96, 8, 8) = onnx::Add(%459, %451), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]\n  %461 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%460, %241), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/Conv2d[conv1]\n  %462 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%461, %242, %243, %244, %245), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/BatchNorm2d[bn1]\n  %463 : Float(1, 576, 8, 8) = onnx::Relu(%462), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]\n  %464 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%463, %247), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/Conv2d[conv2]\n  %465 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%464, %248, %249, %250, %251), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/BatchNorm2d[bn2]\n  %466 : Float(1, 576, 8, 8) = onnx::Relu(%465), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]\n  %467 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%466, %253), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/Conv2d[conv3]\n  %468 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%467, %254, %255, %256, %257), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/BatchNorm2d[bn3]\n  %469 : Float(1, 96, 8, 8) = onnx::Add(%468, %460), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]\n  %470 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%469, %259), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/Conv2d[conv1]\n  %471 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%470, %260, %261, %262, %263), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/BatchNorm2d[bn1]\n  %472 : Float(1, 576, 8, 8) = onnx::Relu(%471), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]\n  %473 : Float(1, 576, 4, 4) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%472, %265), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/Conv2d[conv2]\n  %474 : Float(1, 576, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%473, %266, %267, %268, %269), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/BatchNorm2d[bn2]\n  %475 : Float(1, 576, 4, 4) = onnx::Relu(%474), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]\n  %476 : Float(1, 160, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%475, %271), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/Conv2d[conv3]\n  %477 : Float(1, 160, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%476, %272, %273, %274, %275), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/BatchNorm2d[bn3]\n  %478 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%477, %277), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/Conv2d[conv1]\n  %479 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%478, %278, %279, %280, %281), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/BatchNorm2d[bn1]\n  %480 : Float(1, 960, 4, 4) = onnx::Relu(%479), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]\n  %481 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%480, %283), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/Conv2d[conv2]\n  %482 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%481, %284, %285, %286, %287), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/BatchNorm2d[bn2]\n  %483 : Float(1, 960, 4, 4) = onnx::Relu(%482), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]\n  %484 : Float(1, 160, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%483, %289), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/Conv2d[conv3]\n  %485 : Float(1, 160, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%484, %290, %291, %292, %293), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/BatchNorm2d[bn3]\n  %486 : Float(1, 160, 4, 4) = onnx::Add(%485, %477), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]\n  %487 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%486, %295), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/Conv2d[conv1]\n  %488 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%487, %296, %297, %298, %299), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/BatchNorm2d[bn1]\n  %489 : Float(1, 960, 4, 4) = onnx::Relu(%488), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]\n  %490 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%489, %301), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/Conv2d[conv2]\n  %491 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%490, %302, %303, %304, %305), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/BatchNorm2d[bn2]\n  %492 : Float(1, 960, 4, 4) = onnx::Relu(%491), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]\n  %493 : Float(1, 160, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%492, %307), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/Conv2d[conv3]\n  %494 : Float(1, 160, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%493, %308, %309, %310, %311), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/BatchNorm2d[bn3]\n  %495 : Float(1, 160, 4, 4) = onnx::Add(%494, %486), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]\n  %496 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%495, %313), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Conv2d[conv1]\n  %497 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%496, %314, %315, %316, %317), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/BatchNorm2d[bn1]\n  %498 : Float(1, 960, 4, 4) = onnx::Relu(%497), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]\n  %499 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%498, %319), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Conv2d[conv2]\n  %500 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%499, %320, %321, %322, %323), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/BatchNorm2d[bn2]\n  %501 : Float(1, 960, 4, 4) = onnx::Relu(%500), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]\n  %502 : Float(1, 320, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%501, %325), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Conv2d[conv3]\n  %503 : Float(1, 320, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%502, %326, %327, %328, %329), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/BatchNorm2d[bn3]\n  %504 : Float(1, 320, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%495, %331), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Sequential[shortcut]/Conv2d[0]\n  %505 : Float(1, 320, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%504, %332, %333, %334, %335), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Sequential[shortcut]/BatchNorm2d[1]\n  %506 : Float(1, 320, 4, 4) = onnx::Add(%503, %505), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]\n  %507 : Float(1, 1280, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%506, %337), scope: DataParallel/MobileNetV2[module]/Conv2d[conv2]\n  %508 : Float(1, 1280, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%507, %338, %339, %340, %341), scope: DataParallel/MobileNetV2[module]/BatchNorm2d[bn2]\n  %509 : Float(1, 1280, 4, 4) = onnx::Relu(%508), scope: DataParallel/MobileNetV2[module]\n  %510 : Dynamic = onnx::Pad[mode=constant, pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0](%509), scope: DataParallel/MobileNetV2[module]\n  %511 : Float(1, 1280, 1, 1) = onnx::AveragePool[kernel_shape=[4, 4], pads=[0, 0, 0, 0], strides=[4, 4]](%510), scope: DataParallel/MobileNetV2[module]\n  %512 : Dynamic = onnx::Shape(%511), scope: DataParallel/MobileNetV2[module]\n  %513 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%512), scope: DataParallel/MobileNetV2[module]\n  %514 : Long() = onnx::Squeeze[axes=[0]](%513), scope: DataParallel/MobileNetV2[module]\n  %515 : Long() = onnx::Constant[value={-1}](), scope: DataParallel/MobileNetV2[module]\n  %516 : Dynamic = onnx::Unsqueeze[axes=[0]](%514), scope: DataParallel/MobileNetV2[module]\n  %517 : Dynamic = onnx::Unsqueeze[axes=[0]](%515), scope: DataParallel/MobileNetV2[module]\n  %518 : Dynamic = onnx::Concat[axis=0](%516, %517), scope: DataParallel/MobileNetV2[module]\n  %519 : Float(1, 1280) = onnx::Reshape(%511, %518), scope: DataParallel/MobileNetV2[module]\n  %520 : Float(1, 10) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%519, %343, %344), scope: DataParallel/MobileNetV2[module]/Linear[linear]\n  return (%520);\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7a10c6f279fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'==> Exporting model..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../mobileNetV2.cinic10.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0moperator_export_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOperatorExportTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mONNX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     _export(model, args, f, export_params, verbose, training, input_names, output_names,\n\u001b[0;32m---> 94\u001b[0;31m             operator_export_type=operator_export_type)\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mdefer_weight_export\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexport_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mExportTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROTOBUF_FILE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexport_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_onnx_opset_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefer_weight_export\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_export_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_onnx_opset_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_export_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ONNX export failed: Couldn't export Python operator Scatter\n\nDefined at:\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py(13): scatter_map\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py(15): scatter_map\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py(28): scatter\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py(35): scatter_kwargs\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py(130): scatter\n/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py(119): forward\n/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py(465): _slow_forward\n/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py(475): __call__\n/opt/conda/lib/python3.6/site-packages/torch/jit/__init__.py(109): forward\n/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py(477): __call__\n/opt/conda/lib/python3.6/site-packages/torch/jit/__init__.py(77): get_trace_graph\n/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py(144): _trace_and_get_graph_from_model\n/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py(177): _model_to_graph\n/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py(226): _export\n/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py(94): export\n/opt/conda/lib/python3.6/site-packages/torch/onnx/__init__.py(26): export\n<ipython-input-4-7a10c6f279fd>(47): <module>\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3267): run_code\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3191): run_ast_nodes\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3020): run_cell_async\n/opt/conda/lib/python3.6/site-packages/IPython/core/async_helpers.py(67): _pseudo_sync_runner\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2845): _run_cell\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2819): run_cell\n/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py(536): run_cell\n/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py(294): do_execute\n/opt/conda/lib/python3.6/site-packages/tornado/gen.py(326): wrapper\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py(534): execute_request\n/opt/conda/lib/python3.6/site-packages/tornado/gen.py(326): wrapper\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py(267): dispatch_shell\n/opt/conda/lib/python3.6/site-packages/tornado/gen.py(326): wrapper\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py(357): process_one\n/opt/conda/lib/python3.6/site-packages/tornado/gen.py(1147): run\n/opt/conda/lib/python3.6/site-packages/tornado/gen.py(1233): inner\n/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py(300): null_wrapper\n/opt/conda/lib/python3.6/site-packages/tornado/ioloop.py(758): _run_callback\n/opt/conda/lib/python3.6/asyncio/events.py(145): _run\n/opt/conda/lib/python3.6/asyncio/base_events.py(1434): _run_once\n/opt/conda/lib/python3.6/asyncio/base_events.py(422): run_forever\n/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py(132): start\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py(505): start\n/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py(658): launch_instance\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py(16): <module>\n/opt/conda/lib/python3.6/runpy.py(85): _run_code\n/opt/conda/lib/python3.6/runpy.py(193): _run_module_as_main\n\n\nGraph we tried to export:\ngraph(%0 : Float(1, 3, 32, 32)\n      %1 : Float(32, 3, 3, 3)\n      %2 : Float(32)\n      %3 : Float(32)\n      %4 : Float(32)\n      %5 : Float(32)\n      %6 : Long()\n      %7 : Float(32, 32, 1, 1)\n      %8 : Float(32)\n      %9 : Float(32)\n      %10 : Float(32)\n      %11 : Float(32)\n      %12 : Long()\n      %13 : Float(32, 1, 3, 3)\n      %14 : Float(32)\n      %15 : Float(32)\n      %16 : Float(32)\n      %17 : Float(32)\n      %18 : Long()\n      %19 : Float(16, 32, 1, 1)\n      %20 : Float(16)\n      %21 : Float(16)\n      %22 : Float(16)\n      %23 : Float(16)\n      %24 : Long()\n      %25 : Float(16, 32, 1, 1)\n      %26 : Float(16)\n      %27 : Float(16)\n      %28 : Float(16)\n      %29 : Float(16)\n      %30 : Long()\n      %31 : Float(96, 16, 1, 1)\n      %32 : Float(96)\n      %33 : Float(96)\n      %34 : Float(96)\n      %35 : Float(96)\n      %36 : Long()\n      %37 : Float(96, 1, 3, 3)\n      %38 : Float(96)\n      %39 : Float(96)\n      %40 : Float(96)\n      %41 : Float(96)\n      %42 : Long()\n      %43 : Float(24, 96, 1, 1)\n      %44 : Float(24)\n      %45 : Float(24)\n      %46 : Float(24)\n      %47 : Float(24)\n      %48 : Long()\n      %49 : Float(24, 16, 1, 1)\n      %50 : Float(24)\n      %51 : Float(24)\n      %52 : Float(24)\n      %53 : Float(24)\n      %54 : Long()\n      %55 : Float(144, 24, 1, 1)\n      %56 : Float(144)\n      %57 : Float(144)\n      %58 : Float(144)\n      %59 : Float(144)\n      %60 : Long()\n      %61 : Float(144, 1, 3, 3)\n      %62 : Float(144)\n      %63 : Float(144)\n      %64 : Float(144)\n      %65 : Float(144)\n      %66 : Long()\n      %67 : Float(24, 144, 1, 1)\n      %68 : Float(24)\n      %69 : Float(24)\n      %70 : Float(24)\n      %71 : Float(24)\n      %72 : Long()\n      %73 : Float(144, 24, 1, 1)\n      %74 : Float(144)\n      %75 : Float(144)\n      %76 : Float(144)\n      %77 : Float(144)\n      %78 : Long()\n      %79 : Float(144, 1, 3, 3)\n      %80 : Float(144)\n      %81 : Float(144)\n      %82 : Float(144)\n      %83 : Float(144)\n      %84 : Long()\n      %85 : Float(32, 144, 1, 1)\n      %86 : Float(32)\n      %87 : Float(32)\n      %88 : Float(32)\n      %89 : Float(32)\n      %90 : Long()\n      %91 : Float(192, 32, 1, 1)\n      %92 : Float(192)\n      %93 : Float(192)\n      %94 : Float(192)\n      %95 : Float(192)\n      %96 : Long()\n      %97 : Float(192, 1, 3, 3)\n      %98 : Float(192)\n      %99 : Float(192)\n      %100 : Float(192)\n      %101 : Float(192)\n      %102 : Long()\n      %103 : Float(32, 192, 1, 1)\n      %104 : Float(32)\n      %105 : Float(32)\n      %106 : Float(32)\n      %107 : Float(32)\n      %108 : Long()\n      %109 : Float(192, 32, 1, 1)\n      %110 : Float(192)\n      %111 : Float(192)\n      %112 : Float(192)\n      %113 : Float(192)\n      %114 : Long()\n      %115 : Float(192, 1, 3, 3)\n      %116 : Float(192)\n      %117 : Float(192)\n      %118 : Float(192)\n      %119 : Float(192)\n      %120 : Long()\n      %121 : Float(32, 192, 1, 1)\n      %122 : Float(32)\n      %123 : Float(32)\n      %124 : Float(32)\n      %125 : Float(32)\n      %126 : Long()\n      %127 : Float(192, 32, 1, 1)\n      %128 : Float(192)\n      %129 : Float(192)\n      %130 : Float(192)\n      %131 : Float(192)\n      %132 : Long()\n      %133 : Float(192, 1, 3, 3)\n      %134 : Float(192)\n      %135 : Float(192)\n      %136 : Float(192)\n      %137 : Float(192)\n      %138 : Long()\n      %139 : Float(64, 192, 1, 1)\n      %140 : Float(64)\n      %141 : Float(64)\n      %142 : Float(64)\n      %143 : Float(64)\n      %144 : Long()\n      %145 : Float(384, 64, 1, 1)\n      %146 : Float(384)\n      %147 : Float(384)\n      %148 : Float(384)\n      %149 : Float(384)\n      %150 : Long()\n      %151 : Float(384, 1, 3, 3)\n      %152 : Float(384)\n      %153 : Float(384)\n      %154 : Float(384)\n      %155 : Float(384)\n      %156 : Long()\n      %157 : Float(64, 384, 1, 1)\n      %158 : Float(64)\n      %159 : Float(64)\n      %160 : Float(64)\n      %161 : Float(64)\n      %162 : Long()\n      %163 : Float(384, 64, 1, 1)\n      %164 : Float(384)\n      %165 : Float(384)\n      %166 : Float(384)\n      %167 : Float(384)\n      %168 : Long()\n      %169 : Float(384, 1, 3, 3)\n      %170 : Float(384)\n      %171 : Float(384)\n      %172 : Float(384)\n      %173 : Float(384)\n      %174 : Long()\n      %175 : Float(64, 384, 1, 1)\n      %176 : Float(64)\n      %177 : Float(64)\n      %178 : Float(64)\n      %179 : Float(64)\n      %180 : Long()\n      %181 : Float(384, 64, 1, 1)\n      %182 : Float(384)\n      %183 : Float(384)\n      %184 : Float(384)\n      %185 : Float(384)\n      %186 : Long()\n      %187 : Float(384, 1, 3, 3)\n      %188 : Float(384)\n      %189 : Float(384)\n      %190 : Float(384)\n      %191 : Float(384)\n      %192 : Long()\n      %193 : Float(64, 384, 1, 1)\n      %194 : Float(64)\n      %195 : Float(64)\n      %196 : Float(64)\n      %197 : Float(64)\n      %198 : Long()\n      %199 : Float(384, 64, 1, 1)\n      %200 : Float(384)\n      %201 : Float(384)\n      %202 : Float(384)\n      %203 : Float(384)\n      %204 : Long()\n      %205 : Float(384, 1, 3, 3)\n      %206 : Float(384)\n      %207 : Float(384)\n      %208 : Float(384)\n      %209 : Float(384)\n      %210 : Long()\n      %211 : Float(96, 384, 1, 1)\n      %212 : Float(96)\n      %213 : Float(96)\n      %214 : Float(96)\n      %215 : Float(96)\n      %216 : Long()\n      %217 : Float(96, 64, 1, 1)\n      %218 : Float(96)\n      %219 : Float(96)\n      %220 : Float(96)\n      %221 : Float(96)\n      %222 : Long()\n      %223 : Float(576, 96, 1, 1)\n      %224 : Float(576)\n      %225 : Float(576)\n      %226 : Float(576)\n      %227 : Float(576)\n      %228 : Long()\n      %229 : Float(576, 1, 3, 3)\n      %230 : Float(576)\n      %231 : Float(576)\n      %232 : Float(576)\n      %233 : Float(576)\n      %234 : Long()\n      %235 : Float(96, 576, 1, 1)\n      %236 : Float(96)\n      %237 : Float(96)\n      %238 : Float(96)\n      %239 : Float(96)\n      %240 : Long()\n      %241 : Float(576, 96, 1, 1)\n      %242 : Float(576)\n      %243 : Float(576)\n      %244 : Float(576)\n      %245 : Float(576)\n      %246 : Long()\n      %247 : Float(576, 1, 3, 3)\n      %248 : Float(576)\n      %249 : Float(576)\n      %250 : Float(576)\n      %251 : Float(576)\n      %252 : Long()\n      %253 : Float(96, 576, 1, 1)\n      %254 : Float(96)\n      %255 : Float(96)\n      %256 : Float(96)\n      %257 : Float(96)\n      %258 : Long()\n      %259 : Float(576, 96, 1, 1)\n      %260 : Float(576)\n      %261 : Float(576)\n      %262 : Float(576)\n      %263 : Float(576)\n      %264 : Long()\n      %265 : Float(576, 1, 3, 3)\n      %266 : Float(576)\n      %267 : Float(576)\n      %268 : Float(576)\n      %269 : Float(576)\n      %270 : Long()\n      %271 : Float(160, 576, 1, 1)\n      %272 : Float(160)\n      %273 : Float(160)\n      %274 : Float(160)\n      %275 : Float(160)\n      %276 : Long()\n      %277 : Float(960, 160, 1, 1)\n      %278 : Float(960)\n      %279 : Float(960)\n      %280 : Float(960)\n      %281 : Float(960)\n      %282 : Long()\n      %283 : Float(960, 1, 3, 3)\n      %284 : Float(960)\n      %285 : Float(960)\n      %286 : Float(960)\n      %287 : Float(960)\n      %288 : Long()\n      %289 : Float(160, 960, 1, 1)\n      %290 : Float(160)\n      %291 : Float(160)\n      %292 : Float(160)\n      %293 : Float(160)\n      %294 : Long()\n      %295 : Float(960, 160, 1, 1)\n      %296 : Float(960)\n      %297 : Float(960)\n      %298 : Float(960)\n      %299 : Float(960)\n      %300 : Long()\n      %301 : Float(960, 1, 3, 3)\n      %302 : Float(960)\n      %303 : Float(960)\n      %304 : Float(960)\n      %305 : Float(960)\n      %306 : Long()\n      %307 : Float(160, 960, 1, 1)\n      %308 : Float(160)\n      %309 : Float(160)\n      %310 : Float(160)\n      %311 : Float(160)\n      %312 : Long()\n      %313 : Float(960, 160, 1, 1)\n      %314 : Float(960)\n      %315 : Float(960)\n      %316 : Float(960)\n      %317 : Float(960)\n      %318 : Long()\n      %319 : Float(960, 1, 3, 3)\n      %320 : Float(960)\n      %321 : Float(960)\n      %322 : Float(960)\n      %323 : Float(960)\n      %324 : Long()\n      %325 : Float(320, 960, 1, 1)\n      %326 : Float(320)\n      %327 : Float(320)\n      %328 : Float(320)\n      %329 : Float(320)\n      %330 : Long()\n      %331 : Float(320, 160, 1, 1)\n      %332 : Float(320)\n      %333 : Float(320)\n      %334 : Float(320)\n      %335 : Float(320)\n      %336 : Long()\n      %337 : Float(1280, 320, 1, 1)\n      %338 : Float(1280)\n      %339 : Float(1280)\n      %340 : Float(1280)\n      %341 : Float(1280)\n      %342 : Long()\n      %343 : Float(10, 1280)\n      %344 : Float(10)) {\n  %345 : Float(1, 3, 32, 32) = ^Scatter([0], None, 0)(%0), scope: DataParallel\n  %346 : Float(1, 32, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%345, %1), scope: DataParallel/MobileNetV2[module]/Conv2d[conv1]\n  %347 : Float(1, 32, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%346, %2, %3, %4, %5), scope: DataParallel/MobileNetV2[module]/BatchNorm2d[bn1]\n  %348 : Float(1, 32, 32, 32) = onnx::Relu(%347), scope: DataParallel/MobileNetV2[module]\n  %349 : Float(1, 32, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%348, %7), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Conv2d[conv1]\n  %350 : Float(1, 32, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%349, %8, %9, %10, %11), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/BatchNorm2d[bn1]\n  %351 : Float(1, 32, 32, 32) = onnx::Relu(%350), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]\n  %352 : Float(1, 32, 32, 32) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%351, %13), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Conv2d[conv2]\n  %353 : Float(1, 32, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%352, %14, %15, %16, %17), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/BatchNorm2d[bn2]\n  %354 : Float(1, 32, 32, 32) = onnx::Relu(%353), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]\n  %355 : Float(1, 16, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%354, %19), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Conv2d[conv3]\n  %356 : Float(1, 16, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%355, %20, %21, %22, %23), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/BatchNorm2d[bn3]\n  %357 : Float(1, 16, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%348, %25), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Sequential[shortcut]/Conv2d[0]\n  %358 : Float(1, 16, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%357, %26, %27, %28, %29), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]/Sequential[shortcut]/BatchNorm2d[1]\n  %359 : Float(1, 16, 32, 32) = onnx::Add(%356, %358), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[0]\n  %360 : Float(1, 96, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%359, %31), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Conv2d[conv1]\n  %361 : Float(1, 96, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%360, %32, %33, %34, %35), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/BatchNorm2d[bn1]\n  %362 : Float(1, 96, 32, 32) = onnx::Relu(%361), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]\n  %363 : Float(1, 96, 32, 32) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%362, %37), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Conv2d[conv2]\n  %364 : Float(1, 96, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%363, %38, %39, %40, %41), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/BatchNorm2d[bn2]\n  %365 : Float(1, 96, 32, 32) = onnx::Relu(%364), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]\n  %366 : Float(1, 24, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%365, %43), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Conv2d[conv3]\n  %367 : Float(1, 24, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%366, %44, %45, %46, %47), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/BatchNorm2d[bn3]\n  %368 : Float(1, 24, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%359, %49), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Sequential[shortcut]/Conv2d[0]\n  %369 : Float(1, 24, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%368, %50, %51, %52, %53), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]/Sequential[shortcut]/BatchNorm2d[1]\n  %370 : Float(1, 24, 32, 32) = onnx::Add(%367, %369), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[1]\n  %371 : Float(1, 144, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%370, %55), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/Conv2d[conv1]\n  %372 : Float(1, 144, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%371, %56, %57, %58, %59), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/BatchNorm2d[bn1]\n  %373 : Float(1, 144, 32, 32) = onnx::Relu(%372), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]\n  %374 : Float(1, 144, 32, 32) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%373, %61), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/Conv2d[conv2]\n  %375 : Float(1, 144, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%374, %62, %63, %64, %65), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/BatchNorm2d[bn2]\n  %376 : Float(1, 144, 32, 32) = onnx::Relu(%375), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]\n  %377 : Float(1, 24, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%376, %67), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/Conv2d[conv3]\n  %378 : Float(1, 24, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%377, %68, %69, %70, %71), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]/BatchNorm2d[bn3]\n  %379 : Float(1, 24, 32, 32) = onnx::Add(%378, %370), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[2]\n  %380 : Float(1, 144, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%379, %73), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/Conv2d[conv1]\n  %381 : Float(1, 144, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%380, %74, %75, %76, %77), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/BatchNorm2d[bn1]\n  %382 : Float(1, 144, 32, 32) = onnx::Relu(%381), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]\n  %383 : Float(1, 144, 16, 16) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%382, %79), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/Conv2d[conv2]\n  %384 : Float(1, 144, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%383, %80, %81, %82, %83), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/BatchNorm2d[bn2]\n  %385 : Float(1, 144, 16, 16) = onnx::Relu(%384), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]\n  %386 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%385, %85), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/Conv2d[conv3]\n  %387 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%386, %86, %87, %88, %89), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[3]/BatchNorm2d[bn3]\n  %388 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%387, %91), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/Conv2d[conv1]\n  %389 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%388, %92, %93, %94, %95), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/BatchNorm2d[bn1]\n  %390 : Float(1, 192, 16, 16) = onnx::Relu(%389), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]\n  %391 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%390, %97), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/Conv2d[conv2]\n  %392 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%391, %98, %99, %100, %101), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/BatchNorm2d[bn2]\n  %393 : Float(1, 192, 16, 16) = onnx::Relu(%392), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]\n  %394 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%393, %103), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/Conv2d[conv3]\n  %395 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%394, %104, %105, %106, %107), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]/BatchNorm2d[bn3]\n  %396 : Float(1, 32, 16, 16) = onnx::Add(%395, %387), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[4]\n  %397 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%396, %109), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/Conv2d[conv1]\n  %398 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%397, %110, %111, %112, %113), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/BatchNorm2d[bn1]\n  %399 : Float(1, 192, 16, 16) = onnx::Relu(%398), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]\n  %400 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%399, %115), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/Conv2d[conv2]\n  %401 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%400, %116, %117, %118, %119), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/BatchNorm2d[bn2]\n  %402 : Float(1, 192, 16, 16) = onnx::Relu(%401), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]\n  %403 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%402, %121), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/Conv2d[conv3]\n  %404 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%403, %122, %123, %124, %125), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]/BatchNorm2d[bn3]\n  %405 : Float(1, 32, 16, 16) = onnx::Add(%404, %396), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[5]\n  %406 : Float(1, 192, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%405, %127), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/Conv2d[conv1]\n  %407 : Float(1, 192, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%406, %128, %129, %130, %131), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/BatchNorm2d[bn1]\n  %408 : Float(1, 192, 16, 16) = onnx::Relu(%407), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]\n  %409 : Float(1, 192, 8, 8) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%408, %133), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/Conv2d[conv2]\n  %410 : Float(1, 192, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%409, %134, %135, %136, %137), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/BatchNorm2d[bn2]\n  %411 : Float(1, 192, 8, 8) = onnx::Relu(%410), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]\n  %412 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%411, %139), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/Conv2d[conv3]\n  %413 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%412, %140, %141, %142, %143), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[6]/BatchNorm2d[bn3]\n  %414 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%413, %145), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/Conv2d[conv1]\n  %415 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%414, %146, %147, %148, %149), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/BatchNorm2d[bn1]\n  %416 : Float(1, 384, 8, 8) = onnx::Relu(%415), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]\n  %417 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%416, %151), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/Conv2d[conv2]\n  %418 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%417, %152, %153, %154, %155), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/BatchNorm2d[bn2]\n  %419 : Float(1, 384, 8, 8) = onnx::Relu(%418), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]\n  %420 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%419, %157), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/Conv2d[conv3]\n  %421 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%420, %158, %159, %160, %161), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]/BatchNorm2d[bn3]\n  %422 : Float(1, 64, 8, 8) = onnx::Add(%421, %413), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[7]\n  %423 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%422, %163), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/Conv2d[conv1]\n  %424 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%423, %164, %165, %166, %167), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/BatchNorm2d[bn1]\n  %425 : Float(1, 384, 8, 8) = onnx::Relu(%424), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]\n  %426 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%425, %169), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/Conv2d[conv2]\n  %427 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%426, %170, %171, %172, %173), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/BatchNorm2d[bn2]\n  %428 : Float(1, 384, 8, 8) = onnx::Relu(%427), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]\n  %429 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%428, %175), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/Conv2d[conv3]\n  %430 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%429, %176, %177, %178, %179), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]/BatchNorm2d[bn3]\n  %431 : Float(1, 64, 8, 8) = onnx::Add(%430, %422), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[8]\n  %432 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%431, %181), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/Conv2d[conv1]\n  %433 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%432, %182, %183, %184, %185), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/BatchNorm2d[bn1]\n  %434 : Float(1, 384, 8, 8) = onnx::Relu(%433), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]\n  %435 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%434, %187), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/Conv2d[conv2]\n  %436 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%435, %188, %189, %190, %191), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/BatchNorm2d[bn2]\n  %437 : Float(1, 384, 8, 8) = onnx::Relu(%436), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]\n  %438 : Float(1, 64, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%437, %193), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/Conv2d[conv3]\n  %439 : Float(1, 64, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%438, %194, %195, %196, %197), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]/BatchNorm2d[bn3]\n  %440 : Float(1, 64, 8, 8) = onnx::Add(%439, %431), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[9]\n  %441 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%440, %199), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Conv2d[conv1]\n  %442 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%441, %200, %201, %202, %203), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/BatchNorm2d[bn1]\n  %443 : Float(1, 384, 8, 8) = onnx::Relu(%442), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]\n  %444 : Float(1, 384, 8, 8) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%443, %205), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Conv2d[conv2]\n  %445 : Float(1, 384, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%444, %206, %207, %208, %209), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/BatchNorm2d[bn2]\n  %446 : Float(1, 384, 8, 8) = onnx::Relu(%445), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]\n  %447 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%446, %211), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Conv2d[conv3]\n  %448 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%447, %212, %213, %214, %215), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/BatchNorm2d[bn3]\n  %449 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%440, %217), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Sequential[shortcut]/Conv2d[0]\n  %450 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%449, %218, %219, %220, %221), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]/Sequential[shortcut]/BatchNorm2d[1]\n  %451 : Float(1, 96, 8, 8) = onnx::Add(%448, %450), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[10]\n  %452 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%451, %223), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/Conv2d[conv1]\n  %453 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%452, %224, %225, %226, %227), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/BatchNorm2d[bn1]\n  %454 : Float(1, 576, 8, 8) = onnx::Relu(%453), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]\n  %455 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%454, %229), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/Conv2d[conv2]\n  %456 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%455, %230, %231, %232, %233), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/BatchNorm2d[bn2]\n  %457 : Float(1, 576, 8, 8) = onnx::Relu(%456), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]\n  %458 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%457, %235), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/Conv2d[conv3]\n  %459 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%458, %236, %237, %238, %239), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]/BatchNorm2d[bn3]\n  %460 : Float(1, 96, 8, 8) = onnx::Add(%459, %451), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[11]\n  %461 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%460, %241), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/Conv2d[conv1]\n  %462 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%461, %242, %243, %244, %245), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/BatchNorm2d[bn1]\n  %463 : Float(1, 576, 8, 8) = onnx::Relu(%462), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]\n  %464 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%463, %247), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/Conv2d[conv2]\n  %465 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%464, %248, %249, %250, %251), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/BatchNorm2d[bn2]\n  %466 : Float(1, 576, 8, 8) = onnx::Relu(%465), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]\n  %467 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%466, %253), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/Conv2d[conv3]\n  %468 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%467, %254, %255, %256, %257), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]/BatchNorm2d[bn3]\n  %469 : Float(1, 96, 8, 8) = onnx::Add(%468, %460), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[12]\n  %470 : Float(1, 576, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%469, %259), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/Conv2d[conv1]\n  %471 : Float(1, 576, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%470, %260, %261, %262, %263), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/BatchNorm2d[bn1]\n  %472 : Float(1, 576, 8, 8) = onnx::Relu(%471), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]\n  %473 : Float(1, 576, 4, 4) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%472, %265), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/Conv2d[conv2]\n  %474 : Float(1, 576, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%473, %266, %267, %268, %269), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/BatchNorm2d[bn2]\n  %475 : Float(1, 576, 4, 4) = onnx::Relu(%474), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]\n  %476 : Float(1, 160, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%475, %271), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/Conv2d[conv3]\n  %477 : Float(1, 160, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%476, %272, %273, %274, %275), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[13]/BatchNorm2d[bn3]\n  %478 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%477, %277), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/Conv2d[conv1]\n  %479 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%478, %278, %279, %280, %281), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/BatchNorm2d[bn1]\n  %480 : Float(1, 960, 4, 4) = onnx::Relu(%479), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]\n  %481 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%480, %283), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/Conv2d[conv2]\n  %482 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%481, %284, %285, %286, %287), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/BatchNorm2d[bn2]\n  %483 : Float(1, 960, 4, 4) = onnx::Relu(%482), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]\n  %484 : Float(1, 160, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%483, %289), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/Conv2d[conv3]\n  %485 : Float(1, 160, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%484, %290, %291, %292, %293), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]/BatchNorm2d[bn3]\n  %486 : Float(1, 160, 4, 4) = onnx::Add(%485, %477), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[14]\n  %487 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%486, %295), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/Conv2d[conv1]\n  %488 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%487, %296, %297, %298, %299), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/BatchNorm2d[bn1]\n  %489 : Float(1, 960, 4, 4) = onnx::Relu(%488), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]\n  %490 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%489, %301), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/Conv2d[conv2]\n  %491 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%490, %302, %303, %304, %305), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/BatchNorm2d[bn2]\n  %492 : Float(1, 960, 4, 4) = onnx::Relu(%491), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]\n  %493 : Float(1, 160, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%492, %307), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/Conv2d[conv3]\n  %494 : Float(1, 160, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%493, %308, %309, %310, %311), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]/BatchNorm2d[bn3]\n  %495 : Float(1, 160, 4, 4) = onnx::Add(%494, %486), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[15]\n  %496 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%495, %313), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Conv2d[conv1]\n  %497 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%496, %314, %315, %316, %317), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/BatchNorm2d[bn1]\n  %498 : Float(1, 960, 4, 4) = onnx::Relu(%497), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]\n  %499 : Float(1, 960, 4, 4) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%498, %319), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Conv2d[conv2]\n  %500 : Float(1, 960, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%499, %320, %321, %322, %323), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/BatchNorm2d[bn2]\n  %501 : Float(1, 960, 4, 4) = onnx::Relu(%500), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]\n  %502 : Float(1, 320, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%501, %325), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Conv2d[conv3]\n  %503 : Float(1, 320, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%502, %326, %327, %328, %329), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/BatchNorm2d[bn3]\n  %504 : Float(1, 320, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%495, %331), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Sequential[shortcut]/Conv2d[0]\n  %505 : Float(1, 320, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%504, %332, %333, %334, %335), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]/Sequential[shortcut]/BatchNorm2d[1]\n  %506 : Float(1, 320, 4, 4) = onnx::Add(%503, %505), scope: DataParallel/MobileNetV2[module]/Sequential[layers]/Block[16]\n  %507 : Float(1, 1280, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%506, %337), scope: DataParallel/MobileNetV2[module]/Conv2d[conv2]\n  %508 : Float(1, 1280, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%507, %338, %339, %340, %341), scope: DataParallel/MobileNetV2[module]/BatchNorm2d[bn2]\n  %509 : Float(1, 1280, 4, 4) = onnx::Relu(%508), scope: DataParallel/MobileNetV2[module]\n  %510 : Dynamic = onnx::Pad[mode=constant, pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0](%509), scope: DataParallel/MobileNetV2[module]\n  %511 : Float(1, 1280, 1, 1) = onnx::AveragePool[kernel_shape=[4, 4], pads=[0, 0, 0, 0], strides=[4, 4]](%510), scope: DataParallel/MobileNetV2[module]\n  %512 : Dynamic = onnx::Shape(%511), scope: DataParallel/MobileNetV2[module]\n  %513 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%512), scope: DataParallel/MobileNetV2[module]\n  %514 : Long() = onnx::Squeeze[axes=[0]](%513), scope: DataParallel/MobileNetV2[module]\n  %515 : Long() = onnx::Constant[value={-1}](), scope: DataParallel/MobileNetV2[module]\n  %516 : Dynamic = onnx::Unsqueeze[axes=[0]](%514), scope: DataParallel/MobileNetV2[module]\n  %517 : Dynamic = onnx::Unsqueeze[axes=[0]](%515), scope: DataParallel/MobileNetV2[module]\n  %518 : Dynamic = onnx::Concat[axis=0](%516, %517), scope: DataParallel/MobileNetV2[module]\n  %519 : Float(1, 1280) = onnx::Reshape(%511, %518), scope: DataParallel/MobileNetV2[module]\n  %520 : Float(1, 10) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%519, %343, %344), scope: DataParallel/MobileNetV2[module]/Linear[linear]\n  return (%520);\n}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from models import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "# net = VGG('VGG19')\n",
    "# net = ResNet18()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "net = MobileNetV2()\n",
    "# net = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Load checkpoint.\n",
    "print('==> Resuming from checkpoint..')\n",
    "checkpoint = torch.load('/tmp/work/checkpoint/ckpt.cinic10.0.t7')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "# Export model\n",
    "print('==> Exporting model..')\n",
    "dummy_input = Variable(torch.randn(1, 3, 32, 32), requires_grad=False)\n",
    "torch.onnx.export(net, dummy_input, \"../mobileNetV2.cinic10.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
